# Fairness-Checker
This repository contains the source code, benchmark models, and datasets for the paper - "Modular Fairness Checker for Machine Learning Pipeline"

## Abstract 
Machine learning (ML) systems are increasingly used in high-stakes decision-making processes, raising concerns about fairness and potential biases. Addressing these concerns requires new tools and methods to detect bias in ML-based software effectively. Current bias detection techniques typically analyze global fairness that evaluate the fairness of the ML pipeline holistically. However, this approach has limitations in identifying the pipeline component responsible for bias and explaining the root cause. In addition, developers lack methods to apply modular fairness constraints in design time. We propose a novel approach that leverages concentration inequalities to detect bias in the ML pipeline, focusing on modular fairness. Our method begins by computing fairness scores for the dataset features, highlighting which features are positively or negatively impacted after applying preprocessing components in the pipeline. These insights are then applied to concentration inequalities to detect the fairness bugs within the ML pipeline. Additionally, we developed a programming language abstraction that allows developers to specify and enforce modular fairness during ML pipeline
development. We evaluated our approach using four datasets, two fairness metrics, and 13 ML algorithms. Fairness Checker has a high success rate in detecting fairness bugs. Specifically, our method identified 42 out of
45 buggy cases with an average processing time of 26 seconds per case, outperforming existing bias detection methods, which identified up to 35 out of 45 cases with an average of 31 seconds per case
